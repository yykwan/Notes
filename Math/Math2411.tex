\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx, float}
\usepackage{forest, textcomp, pgfplots, tikz}
\usepackage{ marvosym }
\graphicspath{{images/}}
\pgfplotsset{compat=1.18}

\title{Math2411 Applied Statistics}

\begin{document}


\section{Statistics Basic Framework}
$\rightarrow$ population \\
$\rightarrow$ sample \\
$\rightarrow$ statistics \\
$\rightarrow$ parameter \\
Data = collection of observations of variables;

\subsection{Types of Data}

\begin{forest}
for tree = drawn, rounded corners, align = center
[Data Types
[Categorical [Nominal] [Ordinal] ] [Numerical [Discrete] [Continuous]]]
\end{forest}

\subsection{Descriptive Statistics}
$\rightarrow$ describe data using measures of central tendency or dispersion\\
$\rightarrow$ provides a summary of the data.\\
$\rightarrow$ identify patterns and trends\\
\subsection{Inferential Statistics}
$\rightarrow$ uses sample data to make inferences about a large population\\
$\rightarrow$ test hypotheses about a statement and make decisions\\
$\rightarrow$ address a scientific question\\
\subsection{Frequency table}
$\rightarrow$ graphical representation = histogram \\

\section{Samples}
\subsection{Sample mean}
$\rightarrow$ center of data\\
$\rightarrow$ \[\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i\]
\subsection{Sample variance}
$\rightarrow$ dispersion or spread of data\\
$\rightarrow$ \[
s^2 \text{ or } s^2_{n-1} = \frac{1}{n} \sum_{i=1}^n (x_i -\overline{x})^2 \quad \]
\subsection{Sample standard deviation}
$\rightarrow$ \[s \text{ or } s_{n-1} = \sqrt{s^2_{n-1}}\]
Proof: \\
\[\sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n (x^2_i - 2x_i\overline{x}^2 + \overline{x}^2) = \sum_{i=1}^n x^2_i - 2\sum_{i=1}^n x_i\overline{x} + \sum_{i+1}^n \overline{x}^2\]\\
\[\sum_{i=1}^n x^2_i -2\overline{x}n\overline{x} + nx^2 = \sum_{i=1}^2 x^2_i - n\overline{x}^2\]
Therefore, \[s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2\]
\subsection{Sample median}
$\rightarrow$ if odd $\rightarrow$ median = $\frac{n+1}{2}$ \\
$\rightarrow$ if even $\rightarrow$ median = average of $\frac{n}{2}$ and $\frac{n}{2} +1$ 
\subsection{Quantiles}
$\rightarrow$ also called $p=\frac{x}{100}$ quantile 
\begin{align*}
\rightarrow k = np + 0.5 \rightarrow &\text{if } k = \text{integer} \rightarrow \text{\(k\)-th data point is the data point} \\
&\text{if } k \neq \text{integer} \rightarrow \text{quantile = average of k and k+1}
\end{align*}
Interquatile range (IQR) $ = Q_3 - Q_1$ 
\subsection{Boxplot}
$\rightarrow$ shows center, dispersion, typical range, outliers\\
$\rightarrow$ potential outliers $=[Q_1 - 1.5\times IQR , Q_3 + 1.5\times IQR]$ \\
$\rightarrow$ Steps:
\begin{itemize}
\item[1.]Sort Data
\item[2.]Box: median, $Q_1, Q_3$, IQR
\item[3.] Outliers and whiskers
\end{itemize}

\section{Events, Sets and Probability}
\subsection{Random experiment/trial}
$\rightarrow$ a procedure that generates an uncertain outcome.\\
\subsection{Sample space $\Omega$}
$\rightarrow$ set of all possible outcomes of an experiment \\
\subsection{Event}
$\rightarrow$ a set of outcomes \\
\subsection{Operation rules of events/sets}
\begin{itemize}
\item[1.] Intersection $A \cap B$
\item[2.] Union $A\cup B$
\item[3.] Compliment $A^c$
\item[4.] (set) difference $A-B = A \cap B^c$
\item[5.] Symmetric Difference $A \triangle B = (A-B) \cup (B-A) = A \cup B - (A \cap B)$
\end{itemize}
\subsection{Useful Terms}
\begin{itemize}
\item[1.] Empty events $\emptyset$
\item[2.] Disjoint events $A \cap B = \emptyset$
\item[3.] Mutually exclusive $\rightarrow$ disjoint events $A \cap B = \emptyset$
\item[4.] Exhaustive $\rightarrow$ if union of events $A_1, A_2, ..., A_k$ is $\Omega$
\end{itemize}
\subsection{Commutative Law}
$\rightarrow A \cap B = B \cap A$\\
$\rightarrow A \cup B = B \cup A$
\subsection{Associative Law}
$\rightarrow (A \cap B) \cap C = A \cap (B \cap C)$ \\
$\rightarrow (A \cup B) \cup C = A \cup (B \cup C)$
\subsection{Distributive Law}
$\rightarrow (A \cup B) \cap C = (A \cap C) \cup (B \cap C)$ \\
$\rightarrow (A \cap B ) \cup C = (A \cup C) \cap (B \cup C)$
\subsection{De Morgan's Law}
$\rightarrow (A \cap B)^c = A^c \cup B^c$ \\
$\rightarrow (A \cup B)^c = A^c \cap B^c$
\subsection{Proofing}
\subsubsection{Proofing with Venn Diagram}
\begin{itemize}
    \item[1.] Draw Venn diagram
    \item[2.] Mark number for each slice
    \item[3.] Sub into equation
\end{itemize}
\subsubsection{Proofing with algebraic rules (example)}
$A \triangle B = (A-B) \cup (B-A) = A \cup B - (A \cap B)$\\
$A-B = A \cap B^c , B-A = B \cap A^c$ \\
$(A-B) \cup (B-A) = (A \cap B^c) \cup (B \cap A^c) = [A \cup (B \cap A^c)] \cap [B^C \cup (B \cap A^c)] \\ = [A \cup B] \cap [B^c \cup A^c] = (A \cup B) \cap (A \cap B))^c = A \cup B - (A \cap B)$

\subsection{Axioms of Probability}
If satisfy: \\
\begin{itemize}
\item[1.] $ 0 \leq P(E) \leq 1$ \\
\item[2.] $P(\Omega) =1$ \\
\item[3.] if $E_1,E_2,...$ are mutually exclusive events $P(\cup_{i=1}^n E_i) - \sum_{i=1}^n P(E_i)$ \\
\end{itemize}
$\rightarrow$ Events in $\Omega \rightarrow R$ is a probability function (P) \\
Probability Space: $(\Omega, A, P)$ \\
\subsection{Probability Properties}
$\rightarrow P(A^C) = 1-P(C)$\\
$\rightarrow P(\emptyset)=0$\\
$\rightarrow A \subset B, then P(A) \leq P(B)$\\
$\rightarrow P(A \cup B) = P(A)+P(B)-P(A \cap B)$\\
\subsection{Union Bound}
$P(A \cup B) \leq P(A) +P(B)$ \\
$P(A \cap B) \geq 1-P(A^c) - P(B^c) $ \\
*$P(A \cup B) = P(A) + P(B) -P(A \cap B)$ \\
Example: Prove $P(E)+P(F)-2P(E \cap F) = P(E \triangle F)$ \\
$ E \triangle F = E \cup F - E\cap F$ \\
$E \triangle F$ and $E \cap F$ is non-overlapping and union is $E \cup F$, so $P(E \cup F) = P(E \triangle F) + P(E \cap F)$ \\
$P(E \cup F) = P(E) +P(F) -P(E \cap F)$ \\
Therefore, $P(E \triangle F) = P(E) +P(F) -2P(E \cap F)$ \\
\subsection{Sample space with equally likely outcomes}
$\rightarrow |\Omega| \textless + \infty $\\
$\rightarrow$ If $\Omega$ is a sample space with N number of outcomes, and for all outcomes $w_i,w_j,P(w_i) = P(w_j)$\\
$\rightarrow P(w_i) = \frac{1}{N}, P(E) = \frac{|E|}{N}$
\subsection{Combination Number}
$\rightarrow$ choose k unique items from n items, regardless of the order \\
$\rightarrow$\[ C(n,k) \quad \text{or} \quad \binom{n}{k} =\frac{n \times (n-1) \times ... \times (n-k+1)}{k \times (k-1) \times ... \times 1}\]
\subsection{Conditional Probability}
$\rightarrow$ Consider P(A), given the fact that another even B already occurs \\
$\rightarrow P(B) \textgreater 0$, conditional probability of A given B \\
$\rightarrow$\[ P(A|B) = \frac{P(A \cap B)}{P(B)}\] \\
\subsection{Independence}
$\rightarrow$ If $P(A \cap B) = P(A)P(B) \rightarrow$ event A,B are independent \\
Proof: To show independence, consider $P(A \cap B^C)$\\
$A \cap B $ and $A \cap B^c $ are non-overlapping and their union is $A \cap (B \cup B^c) =A$ 
\begin{align*}
P(A \cap B^c) &= P(A) - P(A \cap B) = P(A) - P(A)P(B) \\
&= P(A)(1 - P(B)) = P(A)P(B^c)
\end{align*}
\subsection{Law of total probability}
$\rightarrow$ For a partition $B_1, B_2,..,B_k$
\[P(A) = P(B_1)P(A|B_1) +P(B_2)P(A|B_2) + ... +P(B_k)P(A|B_k)\]
*partition = mutually exclusive and exhaustive events \\
\subsection{Bayes' Theorem}
$\rightarrow$ If $P(A) \textgreater 0, P(B) \textgreater 0$, then
\[P(B|A) = P(A|B)\frac{P(B)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B)+ P(A|B^c)P(B^c)}\]
\newpage
\section{Random Variable (rv)}
= a function on sample space$\Omega$ that associate a real number $X(w)$ with each element in $\Omega$\\
$\rightarrow$ after experiment $\rightarrow$ take particular value/realization X \\
$\rightarrow$ event $X^{-1} = $ pre=image of function X \\
\[P(a \textless X \leq b) = P(X^{-1}((a,b]))\]\\
$\rightarrow P(Z \leq 1) = \frac{\text{area of lower triangle}}{\text{area of square}} = \frac{1}{2}$\\
\subsection{Probability mass function}
$\rightarrow$ range of a rv (same as the range of function $R_X$) = set of all its possible values \\
$\rightarrow X = $ discrete random variable\\
$\rightarrow$ when range of a rv consist of intervals on real line $\rightarrow$ continuous random variable \\
$\rightarrow$ pmf: $p(x) = P(X =x)$, for each possible value $x\in R_x$\\
\[\text{For all }x\in R_X , 0 \leq p(x) \leq 1 \text{ and } \sum_{x\in R_X} p(x) =1\] 
$\rightarrow$ If satisfy above condition $\rightarrow$ discrete rv \\
\subsection{Cumulative distribution function (cdf)}
$\rightarrow R \rightarrow [0,1] \rightarrow F(x) = P(X \leq x)$\\
$\rightarrow P(a \textless X \leq b) = F(b) -F(a)$\\
\subsection{Properties of cdf}
$\rightarrow$ non-decreasing, $F(-\infty) =0, F(+\infty)=1$\\
$\rightarrow$ right-continuous: $\lim_{x \rightarrow a^+} = F(a)$
\subsection{Population mean/ Expectation}
$\rightarrow$ a discrete rv X is defined as: \\
\[E(X) = \sum_{x \in R_X}x p(x) \]
$\rightarrow$ mean usually denoted by $\mu_x, \mu, EX$\\
$\quad \rightarrow $describes the center of the distribution of rv X\\
Example: X is a discrete rv with range ={-1,0,1}. pmf of X is p(-1) = 0.4, p(0) = 0.2, p(1)=0.4. Find E(X) and E($X^2$)\\
Method 1: Find pmf of $X^2$, range is {0,1}
\begin{align*}
P(X^2 =0) &= P(X=0) = 0.2\\
P(X^2 =1) &= P(X=-1,X=1) -0.8\\
E(X^2) &= 0 \times 0.2 + 1 \times 0.8 = 0.8\\
\end{align*}
Method 2:
\begin{align*}
    E(X^2) &= 0 \times 0.2 + 1 \times 0.8\\
    &= 0 \times P(X=0) + 1 \times (P(X=-1)+P(X=1))\\
    &= 0^2 \times P(X=0) + (-1)^2 \times P(X=-1) + (1)^2 \times P(X=1)\\
\end{align*}
$\rightarrow$ discrete rv X, continuous function g(x):\\
\[E(g(X)) = \sum_{x\in R_X} g(x)p(x)\]\\
\subsection{Properties of E(X)}
$\rightarrow E(aX+b) = aE(X) + b$\\
Proof:
\[E(ax+b) = \sum_{x} (ax+b)p(x) = \sum_{x}(axp(x)+bp(x)) = a \sum_{x} xp(x) + b\sum_{x}p(x) = aE(X)+b\] \\
\subsection{(Population) Variance}
\[Var(X) = \sum_{x\in R_X} (x-\mu)^2p(x) = E((X-\mu)^2)\]
*$\sum_{x\in R_X} x^2 p(x) \textless + \infty$ guarantees $\mu$ and Var(X) exists \\
$\rightarrow$ Var(X) is usually denoted by $\sigma_X^2 , \sigma^2$
$\rightarrow$ describe dispersion of the distribution of rv X\\
$\rightarrow$ Population standard deviation $SD(X) = \sqrt{Var(X)}$\\
\[Var(X) = E(X^2) - (E(X))^2\]
Proof:
\begin{align*}
    E(X-\mu ) &= \sum_{x}(x-\mu )^2p(x) = \sum_{x} (x^2 -2\mu x + \mu^2)p(x) \\ &= \sum_{x} x^2 p(x) - \sum_{x}2\mu x p(x) + \sum_{x}\mu^2 p(x) \\ &= E(X^2) - 2\mu \times \mu + \mu ^2 \\ &= E(X^2) - (E(X))^2 
\end{align*}
\subsection{Properties of Var(X)}
\[Var(aX+b) = a^2 Var(X)\]
Proof: Denote $E(X) = \mu$,
\begin{align*}
    Var(aX+b) &= E[(aX+b - E(aX+b))^2] \\
    &= E[(aX+b - a\mu -b )^2]\\
    &= E(a^2(X-\mu)^2)\\
    &= a^2 E((X-\mu))^2\\
    &= a^2 Var(X)
\end{align*}
\subsection{Probability Density Function (pdf)}
$\rightarrow f(x) \geq 0 , a \leq b$\\
\[P(a \textless X \leq b) = \int_a^b f(x) dx\] if continuous = 1\\
$\rightarrow$ cdf $F(a) = \int_{-\infty}^a f(x) dx $, for any point a where f(a) is continuous, F(a) is differentiable $\frac{dF(x)}{dx} |_{x=a} = f(a) $\\
$P(X=a) = \int_a^a f(x)dx =0$ for any a \\
Example:$f(x) = \begin{cases}
    c(2x - x^2) & \text{if } 0 < x < 2, \\
0 & \text{otherwise.}
\end{cases}$ \\
1. Find c \\
\[1=\int_0^2 c(2x-x^2)dx = c[x^2 - \frac{x^3}{3}]_0^2 = \frac{4c}{3} \rightarrow c=\frac{3}{4}\] \\
2. For any a and b in (0,2], calculate $P(a \leq X \leq b)$\\
\[P(a \leq X \leq b) = \int_a^b \frac{3}{4} (2x-x^2)dx = \frac{1}{4} (3t^2-t^3) \text{ for } 0 \textless t \leq 2\] 
\[F(t)=0, \text{ for } t \leq 0 \text{ and } F(t) =1 \text{ for } t \textgreater 2\]\\
\newpage
\subsection{Brief Summary}
Discrete Random Variables:
\begin{itemize}
    \item [1.] Probability Mass Function p(x)\\
    \[0 \leq p(x) \leq 1 \quad \sum_{x} p(x) =1\]\\
    \item[2.] Cumulative Distribution Function \\
    \[F(x) = \sum_{t\leq x} p(t) = P(X \leq x)\]\\
    \item[3.] Expectation\\
    \[E(X) = \sum_{x} xp(x)\]\\
    \[E(g(X))=\sum_{x}g(x)p(x)\]\\
    \item[4.] Variance\\
    \[Var(X) = \sum_{x}(x-\mu x)^2p(x) = E(X-\mu x)^2\]
\end{itemize}
Continuous Random Variables:
\begin{itemize}
    \item [1.] Probability density function f(x)\\
    \[f(x) \geq 0 \quad \int_{-\infty}^{+\infty} f(x) dx =1\]\\
    * $\frac{d}{dx}cdf=pdf$
    \item [2.] Cumulative distribution function \\
    \[F(x)=\int_{-\infty}^x f(t)dt = P(X \leq x)\]
    \item[3.] Expectation\\
    \[E(X) = \int_{-\infty}^{\infty}xf(x)fx\]\\
    \[E(g(X)) = \int_{-\infty}^{\infty}g(x)f(x)dx\]\\
    \item[4.] Variance \\
    \[Var(X) = \int_{-\infty}^{\infty}(x-\mu x)^2f(x)dx = E(X-\mu x)^2\]\\
\end{itemize}
Properties for both:
\begin{itemize}
    \item [1.] $E(aX+b) = aE(X)+b$
    \item[2.] $Var(X) = E(X^2)-(E(X))^2$
    \item[3.] $Var(aX+b) = a^2 Var(X)$
\end{itemize}
For discrete rvs X and Y:
\[E(X+Y)=E(X)+E(Y)\]
*Also ok for continuous rvs using joint pdf\\
Proof with joint pmf:$p(x,y):= P({X =x} \cap {Y=y})$\\
\subsection{Joint Distribution}
$\rightarrow$ for 2 discrete rvs X, Y with range $R_X, R_Y$\\
$\rightarrow$ joint pmf: $p(x,y) = P({X=x} \cap {Y=y}), x \in R_X, y \in R_Y$\\
$\rightarrow 0 \leq p(x,y) \leq 1$ and $\sum_{x\in R_X, y \in R_Y} p(x,y)=1$\\
$\rightarrow$ Marginal pmf / Marginalization:\\
\[\sum_{y\in R_Y}p(x,y)=P_x(x),\sum_{x\in R_X}p(x,y)=P_Y(y)\]
\subsection{Independence of random variables}
$\rightarrow$ for discrete rvs X, Y, if for any $x \in R_X, y \in R_Y$
\[p(x,y)=P_X(x)P_Y(y) \rightarrow independent\]
$\rightarrow$ if independent and continuous,\\
\[E(f(X)g(Y)) = E(f(X))E(g(Y))\]
*$E(XY) = E(X)E(Y)$\\
*$Var(X+Y) = Var(X) + Var(Y)$\\
\subsection{Properties of E(X) and Var(X) with multiple rvs}
$\rightarrow$ For any rvs,
\begin{itemize}
    \item [1.] $E(X+Y) = E(X) +E(Y)$
    \item[2.] $E(X_1, X_2+...+E_n) = E(X_1) +E(X_2)+...+E(X_n)$
\end{itemize}
$\rightarrow$ For independent rvs,
\begin{itemize}
    \item [1.] $Var(X+Y)= Var(X) +Var(Y)$
    \item [2.] $Var(X_1+X_2+...+X_n) = Var(X_1)+Var(X_2) +...+Var(X_n)$
    \item[3.] $E(XY) = E(X) \times E(Y)$
    \item [4.] $E(f(X)g(Y)) = E(f(X)) \times E(g(Y))$
\end{itemize}
\subsection{Binomial Distribution}
$\rightarrow$ consider n(called size) repeated trials where for each trial the outcome is success/failure, denoted by 1 and 0\\
$\rightarrow$ trials = identical, probability of success = p \\
$\rightarrow$ trials = independent, $i \neq j$\\
$\rightarrow$ number of successes in n trials X = binomial rv, denoted by $X\sim B(n,p)$\\
* $\binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n(n-1)..(n-k+1)}{k(k-1)...1}$\\
Pmf of B(n,p)\\
$\rightarrow x=0,1...,n , p(x)=P(X=x)$\\
\[p(x) = P(X=x)=\binom{n}{x}p^x(1-p)^{n-x}\]
Population mean of B(n,p): E(X) = np\\
Population variance of B(n,p): Var(X) = np(1-p)\\
Bernouli rv B(1,p) : outcome of a single 0,1 trial\\
\subsection{Poisson Distribution}
$\rightarrow$ for large n and small p\\
$\rightarrow n \rightarrow \infty, p\rightarrow 0, np \rightarrow \lambda \rightarrow B(n,p) \rightarrow Pois(\lambda)$\\
Pmf of Pois($\lambda)$:\\
\[P(X=x)=e^{-\lambda}\frac{\lambda^x}{x!} \quad \text{, x=0,1,2,...}\]
Population mean of Pois($\lambda$):E(X) = $\lambda$\\
Population variance of Pois($\lambda)$: Var(X) = $\lambda$\\
Method 1: use pmf \[E(X) = \sum_{k=0}^{\infty}ke^{-\lambda}\frac{\lambda^k}{k!} = \sum_{k=1}^{\infty}ke^{-\lambda}\frac{\lambda^k}{(k-1)!} = \lambda \sum_{k=1}^{\infty}ke^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!} = \lambda \]
Method 2: use binomial rv results \[\lim_{np \rightarrow \lambda}E(X) = \lim_{np \rightarrow \lambda}np =\lambda , \lim_{np \rightarrow \lambda}Var(X) = \lim_{np \rightarrow \lambda, p \rightarrow 0}np(1-p) = \lambda\]
\subsection{Normal Distribution}
pdf of $N(\mu, \sigma^2)$: \[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
For $X \sim N(\mu,\sigma^2), E(X)=\mu, Var(X) = \sigma^2$
\subsection{Standard Normal Distribution N(0,1)}
$\rightarrow \text{cdf and } P(a \textless X \leq b) = \int_a^b f(x)dx$\\
$\rightarrow$ if x is a normal rv, then for any $a\neq0$ and $b$, aX+b is also normally distributed\\
\[E(z) = E(\frac{X-\mu}{\sigma}) = \frac{E(X)-\mu}{\sigma} = 0\]
\[Var(Z) = Var(\frac{X-\mu}{\sigma}) = \frac{1}{\sigma^2}Var(X-\mu) = \frac{1}{\sigma^2}Var(X)=1\]
\[P(a \textless X \leq b) = P(\frac{a-\mu}{\sigma} \textless Z \leq \frac{b-\mu}{\sigma}) = \Phi(\frac{b-\mu}{\sigma})- \Phi(\frac{a-\mu}{\sigma})\]
$\rightarrow \Phi(x)=$cdf of a standard normal rv\\
(Population) quantile function:\\
$\rightarrow$= inverse of cdf\\
$\rightarrow P(X \leq x) =q$\\
\subsection{Student's t distribution}
$\rightarrow$ a family of distributions with a parameter $v \textgreater 0$\\
$\rightarrow$ similar shape as the standard normal distribution N(0,1)\\
$\rightarrow$ when v=1, $\rightarrow$ cauchy distribution\\
$\rightarrow$ when v $\rightarrow +\infty \rightarrow$ standard normal N(0,1)
\subsection{Chi-squared distribution}
$\rightarrow$ parameter $k \textgreater 0$\\
$\rightarrow$ pdf is asymmetric and nonzero only on positive real numbers $R^+$\\
$\rightarrow$ for integer k, $X = Z_1^2 +Z_2^2 +...+Z_k^2$, where $Z_i$ are iid standard normal rvs\\
$\rightarrow$ E(X) = k\\
\newpage
\section{Estimation}
\subsection{Estimator}
$\rightarrow$ for the parameter $\theta$ , denoted by $\hat{\theta}$\\
$\rightarrow$ $\hat{\theta}$ is an rv changing with data
\subsection{Accuracy}
$\rightarrow$ Bias $(\hat{\theta},\theta) = E(\hat{\theta})-\theta$\\
$\rightarrow$ If Bias $(\hat{\theta},\theta) = 0$ for all $\theta$ values, $\hat{\theta}$ is unbiased\\
$\rightarrow$ Consider the expectation/mean of $\hat{\theta}$
\subsection{Precision}
$\rightarrow$ it's variance $\rightarrow$ can compare unbiased estimators\\
$\rightarrow$ precision = $\frac{1}{\sigma^2}$
\subsection{Mean Square Error (MSE)}
$\rightarrow$ combines both accuracy and precision\\
$\rightarrow$ describe the overall error on average of estimator\\
\[MSE(\hat{\theta},\theta)=E[(\hat{\theta}-\theta)^2]=[Bias(\hat{\theta},\theta)]^2+Var(\hat{\theta})\]
\subsection{Unbiased Estimator}
\begin{itemize}
    \item [1.] Sample mean estimator (estimate $\mu$)\\
    $\rightarrow$ for population mean $\mu_X = E(X)$\\
    \[\text{sample's mean } \overline{x}=\frac{X_1+X_2+...+X_n}{n}\]\\
    \[\text{sample's variance }Var(\overline{X)}=\frac{\sigma_X^2}{n}, \quad \sigma_X^2=Var(X)\]
    $\rightarrow$ precision increases with the same sample size n\\
    \item[2.] Sample variance estimator (estimate $\sigma^2$)\\
    $\rightarrow E(s^2)=\sigma_X^2$\\
    \[\text{sample variance }s^2=\frac{1}{n-1}\sum_{i=1}^n(X_i - \overline{X})^2 \]
\end{itemize}
\subsection{Maximum Likelihood Estimator (MLE)}
$\rightarrow$ for parameter(s) $\theta$, defined as:\\
\[\hat{\theta}_{\text{MLE}} = \arg \max_{\theta \in \Theta} \mathcal{L}(\theta; X_1, \dots, X_n)\]\\
$\rightarrow For  X \sim B(m,p):\hat{p}=\frac{\overline{X}}{m}$\\
$\rightarrow For  X \sim Pois(\lambda):\overline{\lambda}=\frac{1}{n}\sum_{i=1}^n X_i$\\
$\rightarrow For X \sim N(\mu,\sigma^2):\hat{\mu}=\overline{X}, \quad \hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})^2$
\subsection{Interval Valued Estimation}
$\rightarrow$ confidence level of an interval (c)\\
\[C=P(\theta \in [\hat{\theta}-a,\hat{\theta}+b]) \text{ or } C=P(\hat{\theta}-a \leq \theta \leq \hat{\theta}+b) = P(\theta -b \leq \hat{\theta} \leq \theta+a)\]\\
$\rightarrow$ for sample distribution:
\begin{itemize}
    \item [1.] Normal $X\sim N(\mu,\sigma^2) \rightarrow \overline{X}\sim N(\mu,\frac{\sigma^2}{n})$
    \item[2.] Binomial $X\sim B(m,p) \rightarrow n\overline{X} \sim B(nm,p)$
    \item[3.] Poisson $X\sim Pois(\lambda) \rightarrow n\overline{X}\sim Pois(n,\lambda)$
\end{itemize}
\subsection{Central Limit Theorem}
$\rightarrow$ for an arbitrary distribution with mean E(X) and Var(X)\\
\[ \lim_{n\rightarrow \infty} \frac{X-E(X)}{\sqrt{Var(X)/n}} \rightarrow N(0,1)\]
$\rightarrow$ implies that for any random variable and large n, the distribution approaches normality\\
$\rightarrow$ for simplicity, assume rv X is normally distributed\\
\subsection{Interval estimator for $\mu$ / 95\% confidence interval (CI)}
\[[\overline{X}-1.96\frac{\sigma}{\sqrt{n}},\overline{X}+1.96\frac{\sigma}{\sqrt{n}}]\]
*satisfies: \[P(\mu \in [\overline{X}-1.96\frac{\sigma}{\sqrt{n}}, \overline{X}+1.96\frac{\sigma}{\sqrt{n}}])=C=0.95\]
\subsection{Confidence Interval (when $\sigma$ is known}
\[[\overline{X}-Z_\frac{\alpha}{2}\frac{\sigma}{\sqrt{n}},\overline{X}+Z_\frac{\alpha}{2}\frac{\sigma}{\sqrt{n}}]\]
*critical value $\alpha = 1-c$ and $Z_\alpha = \phi^{-1}(1-\alpha)$
\subsection{Confidence Interval (when $\sigma$ is unknown)}
$\rightarrow \text{For }X\sim N(\mu,\sigma^2)$ with iid samples,\\
$\frac{\overline{X-\mu}}{s/\sqrt{n}}\sim t_{n-1}$ holds where $t_v$ = student's t distribution with degrees of freedom equal to v\\
\[[\overline{X}-t_{n-1,\frac{\alpha}{2}}\frac{s}{\sqrt{n}},\overline{X}+t_{n-1,\frac{\alpha}{2}}\frac{s}{\sqrt{n}}]\]
\subsection{Confidence Interval for $s^2$}
$\rightarrow \text{For }X\sim N(\mu, \sigma^2)$ with iid samples,\\
$\frac{(n-1)s^2}{\sigma^2}\sim X_{n-1}^2$ holds, where $X_K^2$ is the chi-squared distribution with degrees of freedom equal to k
\subsection{Confidence Interval for $\sigma^2$}
$\rightarrow$ at confidence level $c=1-\alpha$\\
\[[\frac{(n-1)s^2}{X_{n-1,\frac{\alpha}{2}}^2},\frac{(n-1)s^2}{X_{n-1,1- \frac{\alpha}{2}}^2}]\]
\newpage
\section{Hypothesis Testing}
\begin{itemize}
    \item [1.] Binary decisions/statements\\
    $\rightarrow$ Null hypothesis $H_0$\\
    $\rightarrow$ Alternative hypothesis $H_1$
    \item[2.] Test statistic\\
    $\rightarrow$ generally a function of the iid rvs of the samples
    \item[3.] Decision rule\\
    $\rightarrow$ Choose $H_1$: reject null hypothesis\\
    $\rightarrow$ Choose $H_0$: cannot reject null hypothesis
\end{itemize}
\subsection{Error}
$\rightarrow H_0$ true but reject $H_0 \rightarrow$ Type I error($\alpha$)\\
$\rightarrow H_0$ false but did not reject $H_0 \rightarrow$ Type II error($\beta$)\\
Methodology: Choose c such that Type I error probability $\alpha$ equals to a prespecified smaller number\\
$\rightarrow \alpha$ also called significance level
\subsection{Rejection Region}
$\rightarrow X\sim N(\mu, \sigma^2)$, n iid samples $X_1,X_2,...X_n, \sigma^2$ is known\\
$\begin{cases}
    H_0:\mu = \mu_0 \quad \text{reject }H_0\text{ at a significant level }\alpha\\
    H_1: \mu \textgreater \mu_0 \quad \text{if: } \overline{x} \textgreater \mu_0 + Z_{\alpha} \frac{\sigma}{\sqrt{n}}
\end{cases}$
\subsection{Testing $\mu$ when $\sigma^2$ is known}
\begin{itemize}
    \item [1.] One-sided greater test / Right test\\
    $\begin{cases}
        H_0:\mu =\mu_0 \quad \text{reject }H_0\text{ at }\alpha \text{ if } \overline{x}\textgreater \mu_0 + Z_{\alpha}\frac{\sigma}{\sqrt{n}}\\
        H_1:\mu \textgreater \mu_0
    \end{cases}$
    \item[2.] One-sided less test / Left test\\
    $\begin{cases}
        H_0:\mu =\mu_0 \quad \text{reject }H_0\text{ at }\alpha \text{ if } \overline{x}\textless \mu_0 - Z_{\alpha}\frac{\sigma}{\sqrt{n}}\\
        H_1:\mu \textless \mu_0
    \end{cases}$
    \item[3.] Two-sided test\\
    $\begin{cases}
        H_0:\mu =\mu_0 \quad \text{reject }H_0\text{ at }\alpha \text{ if } \overline{x}\textless \mu_0 - Z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \text{ or }\overline{x}\textgreater \mu_0 + Z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\\
        H_1:\mu \neq \mu_0
    \end{cases}$
    \item[4.] Simple test\\
    $\begin{cases}
        H_0: \mu=\mu_0 \text{ rejection same as one-sided test depending on relation between }H_0 \text{ and }H_1\\
        H_1:\mu =\mu_1
    \end{cases}$
\end{itemize}
\subsection{Power}
$\rightarrow$ power = $1-\beta$ (additional assumption of $\mu$ under $H_1$\\
$\rightarrow$ increase with the difference of values of $\mu$ under $H_0$ and $H_1$ : $\triangle \mu = |\mu_1 - \mu_0|$\\
$\rightarrow$ decrease with the value of $\sigma^2$ (given or assumed)\\
$\rightarrow$ increase with the sample size n\\
$\rightarrow$ General calculation, One-sided greater test($\mu_1 \textgreater \mu_0$)\\
$\begin{cases}
    H_0: \mu = \mu_0\\
    H_1: \mu = \mu_1 \textgreater \mu_0
\end{cases}$
\[\text{Power}=P(Z \textgreater -\frac{\mu_1-\mu_0}{\sigma /\sqrt{n}}+Z_{\alpha}\]
\subsection{Testing of $\mu$ when $\sigma^2$ is unknown}
$\begin{cases}
    H_0:\mu =\mu_0 \quad \text{ reject } H_0 \text{ at } \alpha \text{ if } \overline{X} \textgreater \mu_0 + Z_{\alpha} \frac{S_{n-1}}{\sqrt{n}}\\
    H_1:\mu \textgreater \mu_0
\end{cases}$\\
Rejection Region: $\overline{X} \textgreater \mu_0 + t_{n-1,\alpha} \frac{S_{n-1}}{\sqrt{n}}$
\subsection{T-test ($\sigma^2$ is unknown)}
\begin{itemize}
    \item [1.] One-sided greater test / Right test \\
    $\begin{cases}
        H_0: \mu=\mu_0\\
        H_1: \mu_1 \textgreater \mu_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $\frac{\overline{X}-\mu_0}{S_{n-1}/\sqrt{n}} \textgreater t_{n-1,\alpha}$
    \item[2.] One-sided less test / Left test\\
    $\begin{cases}
        H_0:\mu =\mu_0\\
        H_1:\mu \textless \mu_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $|\frac{\overline{x}-\mu_0}{S_{n-1}/\sqrt{n}}| \textgreater t_{n-1, \frac{\alpha}{2}}$
    \item[3.] Two-sided test\\
    $\begin{cases}
        H_0:\mu=\mu_0\\
        H1:\mu \neq \mu_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $|\frac{\overline{x}-\mu_0}{S_{n-1}/\sqrt{n}}| \textgreater t_{n-1, \frac{\alpha}{2}}$
\end{itemize}
* t-statistic: \[T=\frac{\overline{X}-\mu_0}{\sqrt{S_{n-1}^2}/n}\]
* z-statistic: \[Z=\frac{\overline{X}-\mu_0}{\sqrt{\sigma^2/n}}\]
\subsection{p-value}
$\rightarrow$ probability of obtaining a test result that is equal or more extreme than the actually observed result, under $H_0$ 
\begin{itemize}
    \item [1.] Test result: t-statistic / z-statistic
    \item[2.] Actually observed result: test statistic value
    \item[3.] $\begin{aligned}[t]
        \text{More extreme:} & \text{ one-sided greater = t-value is larger} \\
                             & \text{ two-sided test = |t-value| is larger}
    \end{aligned}$
    \item[4.] Probability under null:
    \begin{itemize}
        \item one-sided greater size: $P(T \geq t-value)=p-value$
        \item two-sided test: $P(|T| \geq |t-value|)=2P(T \geq |t-value|) = p-value$
    \end{itemize}
    \item[5.] One-to-one correspondence between t-value and p-value
    \item[6.] Using p-value for hypothesis testing : gives same result as rejection region method
\end{itemize}
\subsection{Power of t-test}
$\rightarrow$ need to assume / be given the sd $\sigma$ to calculate\\
$\rightarrow$ cannot be calculated by formulas\\
$\rightarrow$ use R function power.t.test()\\
$\rightarrow$ delta = difference in means in the direction consistent with the one-sided test, usually $delta \textgreater0$
\subsection{Testing of population variance}
\begin{itemize}
    \item[1.] One-sided greater test / Right test
    $\begin{cases}
        H_0 :\sigma^2=\sigma^2_0\\
        H_1: \sigma^2 \textgreater \sigma_0^2
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $S_{n-1}^2 \textgreater \sigma_0^2 \frac{X_{n-1,\alpha}^2}{n-1}$
    \item[2.] One-sided less test / Left test
    $\begin{cases}
        H_0:\sigma^2 = \sigma^2_0\\
        H_1: \sigma^2 \textless \sigma_0^2
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $S^2_{n-1} \textless \sigma^2_0 \frac{X^2_{n-1,1-\alpha}}{n-1}$
    \item[3.] Two sided test
    $\begin{cases}
        H_0: \sigma^2 = \sigma^2_0\\
        H_1: \sigma^2 \neq \sigma^2_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $S^2_{n-1} \textless \sigma^2_0 \frac{X^2_{n-1,1-\frac{\alpha}{2}}}{n-1}$ or $S^2_{n-1}\textgreater \sigma^2_0 \frac{X^2_{n-1,\frac{\alpha}{2}}}{n-1}$
    \item[4.] Simple test
    $\begin{cases}
        H_0: \sigma^2 = \sigma^2_0\\
        H_1: \sigma^2 = \sigma^2_1
    \end{cases}$\\
    * apply one-sided test method based on the relation between $\sigma^2_0, \sigma^2_1$
\end{itemize}
p-value:\\
Method 1: \[p-value = P(U\textgreater \frac{(n-1)S^2_{n-1}}{\sigma^2_0})\]
Method 2: \[p-value=2\times min[P(U\textgreater\frac{(n-1)S^2_{n-1}}{\sigma^2_0}),P(U\textless\frac{(n-1)S^2_{n-1}}{\sigma^2_0})]\]
\subsection{Two-sided Test}
\begin{itemize}
    \item [1. ] T-test: $\sigma^2$ is unknown\\
    $\begin{cases}
        H_0:\mu = \mu_0\\
        H_1:\mu \neq \mu_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $|\frac{\overline{x}-\mu_0}{S_{n-1}/\sqrt{n}}| \textgreater t_{n-1, \frac{\alpha}{2}}$
    \item[2.] Z-test: $\sigma^2$ is known\\
    $\begin{cases}
        H_0:\mu = \mu_0\\
        H_1: \mu \neq \mu_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $\overline{x} \textless \mu_0 -Z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}$ or $\alpha$ if $\overline{x} \textgreater \mu_0 +Z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}$
    \item[3.] Test for $\sigma^2$\\
    $\begin{cases}
        H_0: \sigma^2 = \sigma^2_0\\
        H_1: \sigma^2 \neq \sigma^2_0
    \end{cases}$\\
    Reject $H_0$ at $\alpha$ if $S^2_{n-1} \textless \sigma^2_0 \frac{X^2_{n-1,1-\frac{\alpha}{2}}}{n-1}$ or $S^2_{n-1} \textgreater \sigma^2_{n-1} \textgreater \sigma^2_0 \frac{X^2_{n-1,\frac{\alpha}{2}}}{n-1}$
\end{itemize}
*For t-test and z-test, if set $C=1-\alpha \rightarrow$ rejection rule $=(\mu \notin CI)$\\
*For $\sigma^2$ test, if set $C=1-\alpha \rightarrow$ rejection rule $=(\sigma^2_0 \notin CI)$
\end{document}


